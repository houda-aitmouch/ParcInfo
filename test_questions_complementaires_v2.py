#!/usr/bin/env python3
"""
Script de test pour valider les am√©liorations du chatbot ParcInfo
avec les 10 questions compl√©mentaires ciblant les lacunes identifi√©es.

Auteur: Assistant IA
Date: 18/08/2025
Version: 2.0
"""

import os
import sys
import django
import time
import json
from datetime import datetime
from typing import Dict, List, Any, Tuple

# Configuration Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ParcInfo.settings')
django.setup()

from apps.chatbot.core_chatbot import ParcInfoChatbot
from apps.chatbot.models import ChatbotFeedback, IntentExample
from apps.materiel_informatique.models import MaterielInformatique
from apps.materiel_bureautique.models import MaterielBureau
from apps.commande_informatique.models import Commande
from apps.commande_bureau.models import CommandeBureau
from apps.fournisseurs.models import Fournisseur
from apps.users.models import CustomUser
from apps.livraison.models import Livraison
from apps.demande_equipement.models import DemandeEquipement

class ChatbotTesterV2:
    """Classe de test avanc√©e pour le chatbot ParcInfo"""
    
    def __init__(self):
        """Initialise le testeur avec le chatbot et les donn√©es de r√©f√©rence"""
        self.chatbot = ParcInfoChatbot()
        self.test_results = []
        self.start_time = time.time()
        
        # Donn√©es de r√©f√©rence pour validation
        self.reference_data = self._load_reference_data()
        
        print("üöÄ Initialisation du testeur de chatbot ParcInfo v2.0")
        print(f"üìä Donn√©es de r√©f√©rence charg√©es: {len(self.reference_data)} entit√©s")
    
    def _load_reference_data(self) -> Dict[str, Any]:
        """Charge les donn√©es de r√©f√©rence depuis la base de donn√©es"""
        try:
            data = {
                'materiels': {},
                'commandes': {},
                'fournisseurs': {},
                'demandes': {},
                'utilisateurs': {}
            }
            
            # Mat√©riels informatiques
            for mat in MaterielInformatique.objects.all():
                data['materiels'][mat.code_inventaire] = {
                    'type': 'informatique',
                    'numero_serie': mat.numero_serie,
                    'designation': mat.designation,
                    'utilisateur': mat.utilisateur.username if mat.utilisateur else None,
                    'commande_id': mat.commande.id if mat.commande else None,
                    'lieu_stockage': getattr(mat, 'lieu_stockage', 'etage1'),
                    'statut': getattr(mat, 'statut', 'affecte')
                }
            
            # Mat√©riels bureautiques
            for mat in MaterielBureau.objects.all():
                data['materiels'][mat.code_inventaire] = {
                    'type': 'bureautique',
                    'numero_serie': mat.numero_serie,
                    'designation': mat.designation,
                    'utilisateur': mat.utilisateur.username if mat.utilisateur else None,
                    'commande_id': mat.commande.id if mat.commande else None,
                    'lieu_stockage': getattr(mat, 'lieu_stockage', 'etage1'),
                    'statut': getattr(mat, 'statut', 'affecte')
                }
            
            # Commandes
            for cmd in Commande.objects.all():
                data['commandes'][cmd.id] = {
                    'type': 'informatique',
                    'montant': float(cmd.montant),
                    'fournisseur': cmd.fournisseur.nom if cmd.fournisseur else None,
                    'date_commande': cmd.date_commande.strftime('%d/%m/%Y'),
                    'garantie_fin': cmd.garantie_fin.strftime('%d/%m/%Y') if cmd.garantie_fin else None
                }
            
            for cmd in CommandeBureau.objects.all():
                data['commandes'][cmd.id] = {
                    'type': 'bureautique',
                    'montant': float(cmd.montant),
                    'fournisseur': cmd.fournisseur.nom if cmd.fournisseur else None,
                    'date_commande': cmd.date_commande.strftime('%d/%m/%Y'),
                    'garantie_fin': cmd.garantie_fin.strftime('%d/%m/%Y') if cmd.garantie_fin else None
                }
            
            # Fournisseurs
            for four in Fournisseur.objects.all():
                data['fournisseurs'][four.nom] = {
                    'ice': four.ice,
                    'localisation': four.localisation
                }
            
            # Demandes
            for dem in DemandeEquipement.objects.all():
                data['demandes'][dem.id] = {
                    'statut': dem.statut,
                    'designation': dem.designation,
                    'demandeur': dem.demandeur.username if dem.demandeur else None
                }
            
            # Utilisateurs
            for user in CustomUser.objects.all():
                data['utilisateurs'][user.username] = {
                    'email': user.email,
                    'role': getattr(user, 'role', 'employe')
                }
            
            return data
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des donn√©es: {e}")
            return {}
    
    def test_question_complementaire(self, question: str, expected_response: Dict[str, Any]) -> Dict[str, Any]:
        """Teste une question compl√©mentaire et valide la r√©ponse"""
        print(f"\nüîç Test: {question[:60]}...")
        
        start_time = time.time()
        
        try:
            # Traitement de la question
            response = self.chatbot.process_query(question)
            
            # Extraction de la r√©ponse textuelle
            if isinstance(response, dict):
                response_text = response.get('response', '')
                intent = response.get('intent', 'unknown')
                confidence = response.get('confidence', 0)
                source = response.get('source', 'unknown')
            else:
                response_text = str(response)
                intent = 'unknown'
                confidence = 0
                source = 'unknown'
            
            response_time = time.time() - start_time
            
            # Validation de la r√©ponse
            validation_result = self._validate_response(
                question, response_text, expected_response, response_time
            )
            
            # Analyse du ton
            tone_analysis = self._analyze_tone(response_text)
            
            # R√©sultat du test
            test_result = {
                'question': question,
                'response': response_text,
                'intent': intent,
                'confidence': confidence,
                'source': source,
                'response_time': response_time,
                'validation': validation_result,
                'tone_analysis': tone_analysis,
                'timestamp': datetime.now().isoformat()
            }
            
            self.test_results.append(test_result)
            
            # Affichage du r√©sultat
            self._display_test_result(test_result)
            
            return test_result
            
        except Exception as e:
            error_result = {
                'question': question,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            self.test_results.append(error_result)
            print(f"‚ùå Erreur lors du test: {e}")
            return error_result
    
    def _validate_response(self, question: str, response: str, expected: Dict[str, Any], response_time: float) -> Dict[str, Any]:
        """Valide la r√©ponse selon les crit√®res attendus"""
        validation = {
            'precision': 0.0,
            'completeness': 0.0,
            'performance': 0.0,
            'overall_score': 0.0,
            'details': []
        }
        
        # Validation de la performance
        if response_time < 2.0:
            validation['performance'] = 1.0
            validation['details'].append("‚úÖ Performance: <2s")
        else:
            validation['performance'] = max(0, 1 - (response_time - 2) / 3)
            validation['details'].append(f"‚ö†Ô∏è Performance: {response_time:.2f}s (>2s)")
        
        # Validation de la pr√©cision
        precision_score = self._validate_precision(question, response, expected)
        validation['precision'] = precision_score
        
        # Validation de la compl√©tude
        completeness_score = self._validate_completeness(question, response, expected)
        validation['completeness'] = completeness_score
        
        # Score global
        validation['overall_score'] = (
            validation['precision'] * 0.5 +
            validation['completeness'] * 0.3 +
            validation['performance'] * 0.2
        )
        
        return validation
    
    def _validate_precision(self, question: str, response: str, expected: Dict[str, Any]) -> float:
        """Valide la pr√©cision des informations dans la r√©ponse"""
        score = 0.0
        details = []
        
        # V√©rification des entit√©s cl√©s
        if 'materiels' in expected:
            for mat_code in expected['materiels']:
                if mat_code in response:
                    score += 0.2
                    details.append(f"‚úÖ Mat√©riel {mat_code} trouv√©")
                else:
                    details.append(f"‚ùå Mat√©riel {mat_code} manquant")
        
        if 'fournisseurs' in expected:
            for four_name in expected['fournisseurs']:
                if four_name in response:
                    score += 0.2
                    details.append(f"‚úÖ Fournisseur {four_name} trouv√©")
                else:
                    details.append(f"‚ùå Fournisseur {four_name} manquant")
        
        if 'montants' in expected:
            for montant in expected['montants']:
                if str(montant) in response:
                    score += 0.2
                    details.append(f"‚úÖ Montant {montant} trouv√©")
                else:
                    details.append(f"‚ùå Montant {montant} manquant")
        
        # V√©rification des relations
        if 'relations' in expected:
            for relation in expected['relations']:
                if relation in response:
                    score += 0.1
                    details.append(f"‚úÖ Relation {relation} trouv√©e")
                else:
                    details.append(f"‚ùå Relation {relation} manquante")
        
        return min(score, 1.0)
    
    def _validate_completeness(self, question: str, response: str, expected: Dict[str, Any]) -> float:
        """Valide la compl√©tude de la r√©ponse"""
        score = 0.0
        details = []
        
        # V√©rification de la structure
        if 'Salut' in response or 'Bonjour' in response:
            score += 0.2
            details.append("‚úÖ Introduction engageante")
        else:
            details.append("‚ùå Introduction manquante")
        
        if '?' in response or 'Besoin' in response or 'Veux-tu' in response:
            score += 0.2
            details.append("‚úÖ Invitation √† poursuivre")
        else:
            details.append("‚ùå Invitation manquante")
        
        # V√©rification des d√©tails
        if len(response.split()) > 20:
            score += 0.2
            details.append("‚úÖ R√©ponse d√©taill√©e")
        else:
            details.append("‚ùå R√©ponse trop courte")
        
        # V√©rification du format
        if '‚Ä¢' in response or '-' in response or '|' in response:
            score += 0.2
            details.append("‚úÖ Format structur√©")
        else:
            details.append("‚ùå Format non structur√©")
        
        # V√©rification de la clart√©
        if 'num√©ro de s√©rie' in response and 'num√©ro de num√©ro de s√©rie' not in response:
            score += 0.2
            details.append("‚úÖ Pas de r√©p√©tition")
        else:
            details.append("‚ùå R√©p√©tition d√©tect√©e")
        
        return min(score, 1.0)
    
    def _analyze_tone(self, response: str) -> Dict[str, Any]:
        """Analyse le ton de la r√©ponse"""
        tone_analysis = {
            'human_like': 0.0,
            'professional': 0.0,
            'friendly': 0.0,
            'technical': 0.0,
            'issues': []
        }
        
        # Analyse du ton humain
        friendly_indicators = ['Salut', 'Bonjour', 'Super', 'Parfait', 'Veux-tu', 'Besoin']
        professional_indicators = ['Veuillez', 'Malheureusement', 'D√©sol√©']
        technical_indicators = ['SQL', 'requ√™te', 'base de donn√©es', 'jointure']
        
        friendly_count = sum(1 for indicator in friendly_indicators if indicator in response)
        professional_count = sum(1 for indicator in professional_indicators if indicator in response)
        technical_count = sum(1 for indicator in technical_indicators if indicator in response)
        
        # Calcul des scores
        total_indicators = len(friendly_indicators) + len(professional_indicators) + len(technical_indicators)
        
        if friendly_count > 0:
            tone_analysis['friendly'] = min(1.0, friendly_count / 3)
            tone_analysis['human_like'] += 0.4
        
        if professional_count > 0:
            tone_analysis['professional'] = min(1.0, professional_count / 3)
            tone_analysis['human_like'] += 0.3
        
        if technical_count > 0:
            tone_analysis['technical'] = min(1.0, technical_count / 3)
            tone_analysis['human_like'] += 0.2
        
        # D√©tection des probl√®mes
        if 'num√©ro de num√©ro de s√©rie' in response:
            tone_analysis['issues'].append("R√©p√©tition d√©tect√©e")
        
        if response.count('Super') > 2 or response.count('Parfait') > 2:
            tone_analysis['issues'].append("Ton r√©p√©titif")
        
        if len(response.split('.')) > 5:
            tone_analysis['issues'].append("R√©ponse trop verbeuse")
        
        return tone_analysis
    
    def _display_test_result(self, result: Dict[str, Any]):
        """Affiche le r√©sultat d'un test"""
        if 'error' in result:
            print(f"‚ùå ERREUR: {result['error']}")
            return
        
        validation = result['validation']
        tone = result['tone_analysis']
        
        print(f"üìù R√©ponse: {result['response'][:100]}...")
        print(f"üéØ Intent: {result['intent']} (confiance: {result['confidence']:.1f})")
        print(f"‚ö° Source: {result['source']} - Temps: {result['response_time']:.2f}s")
        print(f"üìä Score global: {validation['overall_score']:.2f}/1.0")
        print(f"   ‚Ä¢ Pr√©cision: {validation['precision']:.2f}/1.0")
        print(f"   ‚Ä¢ Compl√©tude: {validation['completeness']:.2f}/1.0")
        print(f"   ‚Ä¢ Performance: {validation['performance']:.2f}/1.0")
        
        if tone['issues']:
            print(f"‚ö†Ô∏è Probl√®mes de ton: {', '.join(tone['issues'])}")
        
        for detail in validation['details'][:3]:  # Afficher les 3 premiers d√©tails
            print(f"   {detail}")
    
    def run_all_tests(self):
        """Ex√©cute tous les tests des questions compl√©mentaires"""
        print("\n" + "="*80)
        print("üß™ EX√âCUTION DES TESTS DES QUESTIONS COMPL√âMENTAIRES")
        print("="*80)
        
        # Questions compl√©mentaires avec r√©ponses attendues
        test_cases = [
            {
                'question': "Quels mat√©riels sont stock√©s √† l'√©tage 1 ?",
                'expected': {
                    'materiels': ['cd12', 'cd13', 'cd14', 'ADD/INFO/010', 'ADD/INFO/01000'],
                    'relations': ['√©tage 1', 'stockage']
                }
            },
            {
                'question': "Quelles demandes sont associ√©es √† la comande BC23 ?",
                'expected': {
                    'materiels': ['cd14'],
                    'relations': ['BC23', 'demande n¬∞11']
                }
            },
            {
                'question': "Quel est le statut des livraisons pour le fourniseur 3STD ?",
                'expected': {
                    'fournisseurs': ['3STD'],
                    'relations': ['commande 123', 'retard']
                }
            },
            {
                'question': "Quels mat√©riels informatiques sont marqu√©s comme nouveaux ?",
                'expected': {
                    'materiels': ['ADD/INFO/010', 'ADD/INFO/01000'],
                    'relations': ['nouveaux', 'informatiques']
                }
            },
            {
                'question': "Quel est le total des comandes pass√©es en juillet 2025 ?",
                'expected': {
                    'montants': ['6333', '3333', '2500', '500'],
                    'relations': ['juillet 2025']
                }
            },
            {
                'question': "Quels mat√©riels sont li√©s √† des demandes non approuv√©es ?",
                'expected': {
                    'relations': ['demandes non approuv√©es']
                }
            },
            {
                'question': "Quel fourniseur a livr√© le mat√©riel ADD/INFO/01094 ?",
                'expected': {
                    'fournisseurs': ['AEBDM'],
                    'materiels': ['ADD/INFO/01094']
                }
            },
            {
                'question': "Quels mat√©riels sont publics √† l'√©tage 1 ?",
                'expected': {
                    'materiels': ['cd12', 'cd13', 'cd14', 'ADD/INFO/010', 'ADD/INFO/01000'],
                    'relations': ['publics', '√©tage 1']
                }
            },
            {
                'question': "Combien de mat√©riels sont en stock actuellement ?",
                'expected': {
                    'materiels': ['ADD/INFO/010', 'ADD/INFO/01000'],
                    'relations': ['en stock']
                }
            },
            {
                'question': "Quel mat√©riel est associ√© au fourniseur INCONNU ?",
                'expected': {
                    'relations': ['fournisseur inexistant']
                }
            }
        ]
        
        # Ex√©cution des tests
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n{'='*60}")
            print(f"üß™ TEST {i}/10: {test_case['question']}")
            print(f"{'='*60}")
            
            self.test_question_complementaire(
                test_case['question'], 
                test_case['expected']
            )
            
            # Pause entre les tests
            time.sleep(1)
        
        # G√©n√©ration du rapport final
        self._generate_final_report()
    
    def _generate_final_report(self):
        """G√©n√®re le rapport final des tests"""
        print("\n" + "="*80)
        print("üìä RAPPORT FINAL DES TESTS")
        print("="*80)
        
        if not self.test_results:
            print("‚ùå Aucun r√©sultat de test disponible")
            return
        
        # Statistiques globales
        successful_tests = [r for r in self.test_results if 'error' not in r]
        failed_tests = [r for r in self.test_results if 'error' in r]
        
        if successful_tests:
            avg_precision = sum(r['validation']['precision'] for r in successful_tests) / len(successful_tests)
            avg_completeness = sum(r['validation']['completeness'] for r in successful_tests) / len(successful_tests)
            avg_performance = sum(r['validation']['performance'] for r in successful_tests) / len(successful_tests)
            avg_overall = sum(r['validation']['overall_score'] for r in successful_tests) / len(successful_tests)
            avg_response_time = sum(r['response_time'] for r in successful_tests) / len(successful_tests)
            
            print(f"‚úÖ Tests r√©ussis: {len(successful_tests)}/10")
            print(f"‚ùå Tests √©chou√©s: {len(failed_tests)}/10")
            print(f"üìä Scores moyens:")
            print(f"   ‚Ä¢ Pr√©cision: {avg_precision:.2f}/1.0")
            print(f"   ‚Ä¢ Compl√©tude: {avg_completeness:.2f}/1.0")
            print(f"   ‚Ä¢ Performance: {avg_performance:.2f}/1.0")
            print(f"   ‚Ä¢ Global: {avg_overall:.2f}/1.0")
            print(f"‚ö° Temps de r√©ponse moyen: {avg_response_time:.2f}s")
            
            # Analyse des probl√®mes
            all_issues = []
            for result in successful_tests:
                if result['tone_analysis']['issues']:
                    all_issues.extend(result['tone_analysis']['issues'])
            
            if all_issues:
                print(f"\n‚ö†Ô∏è Probl√®mes d√©tect√©s:")
                issue_counts = {}
                for issue in all_issues:
                    issue_counts[issue] = issue_counts.get(issue, 0) + 1
                
                for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
                    print(f"   ‚Ä¢ {issue}: {count} occurrence(s)")
            
            # Recommandations
            print(f"\nüí° Recommandations:")
            if avg_precision < 0.8:
                print("   ‚Ä¢ Am√©liorer la pr√©cision des r√©ponses (donn√©es incorrectes)")
            if avg_completeness < 0.7:
                print("   ‚Ä¢ Am√©liorer la compl√©tude (d√©tails manquants)")
            if avg_performance < 0.9:
                print("   ‚Ä¢ Optimiser les performances (temps de r√©ponse)")
            if avg_overall >= 0.8:
                print("   ‚Ä¢ Excellent! Le chatbot r√©pond aux exigences")
            elif avg_overall >= 0.6:
                print("   ‚Ä¢ Bon niveau, quelques am√©liorations n√©cessaires")
            else:
                print("   ‚Ä¢ Am√©liorations importantes n√©cessaires")
        
        # Sauvegarde des r√©sultats
        self._save_test_results()
        
        total_time = time.time() - self.start_time
        print(f"\n‚è±Ô∏è Temps total d'ex√©cution: {total_time:.2f}s")
    
    def _save_test_results(self):
        """Sauvegarde les r√©sultats des tests"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"test_results_complementaires_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"üíæ R√©sultats sauvegard√©s dans: {filename}")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde: {e}")


def main():
    """Fonction principale"""
    print("üöÄ Testeur de Chatbot ParcInfo - Questions Compl√©mentaires v2.0")
    print("üìÖ Date: 18/08/2025")
    print("üéØ Objectif: Valider les am√©liorations du chatbot")
    
    try:
        # Initialisation du testeur
        tester = ChatbotTesterV2()
        
        # Ex√©cution des tests
        tester.run_all_tests()
        
        print("\n‚úÖ Tests termin√©s avec succ√®s!")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Tests interrompus par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
