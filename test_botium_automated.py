#!/usr/bin/env python3
"""
Script de test automatis√© Botium-like pour le chatbot ParcInfo
Test des 9 questions identifi√©es comme n√©cessitant des am√©liorations
Focus : Pr√©cision ET ton humain selon le nouveau prompt
"""

import os
import sys
import django
from datetime import datetime
import time
import json

# Configuration Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ParcInfo.settings')
django.setup()

from apps.chatbot.core_chatbot import ParcInfoChatbot

class BotiumTestSuite:
    """Suite de tests automatis√©s pour √©valuer la pr√©cision et le ton humain"""
    
    def __init__(self):
        self.chatbot = None
        self.test_results = []
        self.start_time = None
        
    def setup(self):
        """Initialise le chatbot et la suite de tests"""
        try:
            print("üöÄ Initialisation de la suite de tests Botium...")
            self.chatbot = ParcInfoChatbot()
            print("‚úÖ Chatbot initialis√© avec succ√®s")
            return True
        except Exception as e:
            print(f"‚ùå Erreur d'initialisation: {e}")
            return False
    
    def evaluate_precision(self, response: str, expected_elements: list) -> dict:
        """√âvalue la pr√©cision de la r√©ponse selon les √©l√©ments attendus"""
        score = 0
        found_elements = []
        missing_elements = []
        
        response_lower = response.lower()
        
        for element in expected_elements:
            if element.lower() in response_lower:
                score += 1
                found_elements.append(element)
            else:
                missing_elements.append(element)
        
        precision_score = score / len(expected_elements) if expected_elements else 0
        
        return {
            'score': precision_score,
            'found': found_elements,
            'missing': missing_elements,
            'is_accurate': precision_score >= 0.8
        }
    
    def evaluate_human_tone(self, response: str) -> dict:
        """√âvalue le ton humain selon les directives du prompt"""
        score = 0
        issues = []
        improvements = []
        
        response_lower = response.lower()
        
        # V√©rifier l'introduction engageante (directive 2 du prompt)
        if any(greeting in response_lower for greeting in ['bonjour', 'salut', 'j\'ai v√©rifi√©', 'j\'ai trouv√©']):
            score += 2
        else:
            issues.append("Manque d'introduction engageante")
            improvements.append("Ajouter 'Bonjour ! J'ai v√©rifi√© pour vous'")
        
        # V√©rifier l'absence de termes techniques (directive 2 du prompt)
        technical_terms = ['it', 'cmd', 'fin', 's√©rie', 'n/a']
        for term in technical_terms:
            if term in response_lower:
                score -= 1
                issues.append(f"Terme technique '{term}' pr√©sent")
                improvements.append(f"Remplacer '{term}' par un terme clair")
        
        # V√©rifier l'invitation √† poursuivre (directive 2 du prompt)
        if any(engagement in response_lower for engagement in ['voulez-vous', 'avez-vous', 'besoin', 'souhaitez-vous']):
            score += 2
        else:
            issues.append("Manque d'invitation √† poursuivre")
            improvements.append("Ajouter une question d'engagement")
        
        # V√©rifier la structure claire (directive 4 du prompt)
        if len(response.split('.')) > 2:
            score += 1
        else:
            issues.append("Structure de r√©ponse trop simple")
            improvements.append("Structurer avec introduction, d√©tails, conclusion")
        
        # Normaliser le score sur 5
        score = max(0, min(5, score))
        
        return {
            'score': score,
            'issues': issues,
            'improvements': improvements,
            'is_human': score >= 3
        }
    
    def run_single_test(self, test_name: str, query: str, expected_elements: list, timeout: int = 5) -> dict:
        """Ex√©cute un test individuel"""
        print(f"\nüîç Test : {test_name}")
        print(f"üìù Question : {query}")
        print("-" * 50)
        
        start_time = time.time()
        
        try:
            # Ex√©cuter la requ√™te avec timeout
            response = self.chatbot.process_query(query)
            response_time = time.time() - start_time
            
            # Extraire le texte de la r√©ponse
            if isinstance(response, dict):
                response_text = response.get('response', 'N/A')
                intent = response.get('intent', 'N/A')
                confidence = response.get('confidence', 'N/A')
            else:
                response_text = response
                intent = 'N/A'
                confidence = 'N/A'
            
            # √âvaluer la pr√©cision
            precision_eval = self.evaluate_precision(response_text, expected_elements)
            
            # √âvaluer le ton humain
            tone_eval = self.evaluate_human_tone(response_text)
            
            # V√©rifier le timeout
            timeout_ok = response_time < timeout
            
            # R√©sultats du test
            test_result = {
                'test_name': test_name,
                'query': query,
                'response': response_text,
                'intent': intent,
                'confidence': confidence,
                'response_time': response_time,
                'timeout_ok': timeout_ok,
                'precision': precision_eval,
                'tone': tone_eval,
                'overall_score': (precision_eval['score'] + tone_eval['score'] / 5) / 2
            }
            
            # Affichage des r√©sultats
            print(f"üìù R√©ponse : {response_text[:100]}{'...' if len(response_text) > 100 else ''}")
            print(f"üéØ Intent : {intent}")
            print(f"üìä Confiance : {confidence}%")
            print(f"‚è±Ô∏è  Temps : {response_time:.2f}s")
            print(f"üìä Pr√©cision : {precision_eval['score']:.2f} ({'‚úÖ' if precision_eval['is_accurate'] else '‚ùå'})")
            print(f"üé≠ Ton humain : {tone_eval['score']}/5 ({'‚úÖ' if tone_eval['is_human'] else '‚ùå'})")
            
            if precision_eval['missing']:
                print(f"‚ö†Ô∏è  √âl√©ments manquants : {', '.join(precision_eval['missing'])}")
            
            if tone_eval['issues']:
                print(f"‚ö†Ô∏è  Probl√®mes de ton : {', '.join(tone_eval['issues'])}")
            
            if not timeout_ok:
                print(f"‚ö†Ô∏è  Timeout d√©pass√© ({response_time:.2f}s > {timeout}s)")
            
            return test_result
            
        except Exception as e:
            print(f"‚ùå Erreur lors du test : {e}")
            return {
                'test_name': test_name,
                'query': query,
                'error': str(e),
                'overall_score': 0
            }
    
    def run_test_suite(self):
        """Ex√©cute la suite compl√®te de tests"""
        if not self.setup():
            return False
        
        print("\nüß™ D√©marrage de la suite de tests automatis√©s")
        print("=" * 60)
        
        self.start_time = datetime.now()
        
        # D√©finition des tests avec √©l√©ments attendus
        test_cases = [
            {
                'name': 'Mat√©riels pour superadmin avec demandes',
                'query': "Quel mat√©riel a √©t√© affect√© √† la demande de 'superadmin' ?",
                'expected': ['Demande n¬∞', 'cd14', 'sn14', 'BC23', 'Cam√©ra', 'Casque', 'Baie']
            },
            {
                'name': 'Mat√©riels informatiques sous garantie (d√©tails)',
                'query': "Quels mat√©riels informatiques sont encore sous garantie ?",
                'expected': ['cd12', 'cd13', 'cd14', 'ADD/INFO/010', 'designation', 'utilisateur', 'num√©ro de s√©rie']
            },
            {
                'name': 'Garantie ADD/INFO/01094 (d√©tails)',
                'query': "La garantie du mat√©riel avec le code d'inventaire ADD/INFO/01094 est-elle toujours active ?",
                'expected': ['ADD/INFO/01094', 'AOO2025', 'Armoire', 'jours restants', '28/07/2026']
            },
            {
                'name': 'Mat√©riels bureautiques expirant bient√¥t (contexte)',
                'query': "Liste les mat√©riels bureautiques dont la garantie expire bient√¥t.",
                'expected': ['30 jours', 'mat√©riels actifs', 'contexte', 'ADD/INFO/01094']
            },
            {
                'name': 'Codes d\'inventaire Baie (utilisateurs)',
                'query': "Quel est le code d'inventaire de la Baie ?",
                'expected': ['cd12', 'cd13', 'cd14', 'utilisateur', 'employe anonyme', 'gestionnaire bureau', 'superadmin']
            },
            {
                'name': 'Commandes sans garantie (dates)',
                'query': "Y a-t-il des commandes sans garantie sp√©cifi√©e ?",
                'expected': ['AOO2025', '123', 'BC23', 'expire', '28/07/2026', '15/09/2025', '23/08/2025']
            },
            {
                'name': 'Commandes bureautiques garantie ann√©es (conversationnel)',
                'query': "Liste des commandes bureautiques avec garantie en ann√©es.",
                'expected': ['AOO2025', '1 an', '28/07/2026', 'active', 'conversationnel']
            },
            {
                'name': 'Types de mat√©riels (designations sp√©cifiques)',
                'query': "Quels types de mat√©riels sont disponibles ?",
                'expected': ['Baie', 'Call Server', 'Armoire', 'C√¢ble', 'cd12', 'cd13', 'cd14', 'ADD/INFO/010']
            },
            {
                'name': 'Mat√©riel sn12 avec garantie (d√©tails)',
                'query': "Quel mat√©riel informatique avec le num√©ro de s√©rie sn12 a une garantie associ√©e ?",
                'expected': ['cd12', 'BC23', '23/08/2025', 'jours restants', '5 jours']
            },
            {
                'name': 'Mat√©riels superadmin sous garantie (d√©tails)',
                'query': "Y a-t-il des mat√©riels affect√©s √† 'superadmin' encore sous garantie ?",
                'expected': ['cd14', 'sn14', 'Baie', '23/08/2025', '5 jours restants', 'BC23']
            }
        ]
        
        # Ex√©cution des tests
        for test_case in test_cases:
            result = self.run_single_test(
                test_case['name'],
                test_case['query'],
                test_case['expected']
            )
            self.test_results.append(result)
        
        # G√©n√©ration du rapport
        self.generate_report()
        
        return True
    
    def generate_report(self):
        """G√©n√®re un rapport d√©taill√© des tests"""
        print("\nüìä RAPPORT FINAL DES TESTS")
        print("=" * 60)
        
        if not self.test_results:
            print("‚ùå Aucun r√©sultat de test disponible")
            return
        
        # Calcul des statistiques
        total_tests = len(self.test_results)
        successful_tests = len([r for r in self.test_results if 'error' not in r])
        failed_tests = total_tests - successful_tests
        
        # Scores moyens
        precision_scores = [r['precision']['score'] for r in self.test_results if 'precision' in r]
        tone_scores = [r['tone']['score'] for r in self.test_results if 'tone' in r]
        overall_scores = [r['overall_score'] for r in self.test_results if 'overall_score' in r]
        
        avg_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0
        avg_tone = sum(tone_scores) / len(tone_scores) if tone_scores else 0
        avg_overall = sum(overall_scores) / len(overall_scores) if overall_scores else 0
        
        # Temps de r√©ponse
        response_times = [r['response_time'] for r in self.test_results if 'response_time' in r]
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        
        # Affichage des statistiques
        print(f"üìà Statistiques g√©n√©rales :")
        print(f"   ‚Ä¢ Tests ex√©cut√©s : {total_tests}")
        print(f"   ‚Ä¢ Tests r√©ussis : {successful_tests}")
        print(f"   ‚Ä¢ Tests √©chou√©s : {failed_tests}")
        print(f"   ‚Ä¢ Taux de succ√®s : {(successful_tests/total_tests)*100:.1f}%")
        
        print(f"\nüìä Scores moyens :")
        print(f"   ‚Ä¢ Pr√©cision : {avg_precision:.2f}/1.0")
        print(f"   ‚Ä¢ Ton humain : {avg_tone:.1f}/5.0")
        print(f"   ‚Ä¢ Score global : {avg_overall:.2f}/1.0")
        
        print(f"\n‚è±Ô∏è  Performance :")
        print(f"   ‚Ä¢ Temps de r√©ponse moyen : {avg_response_time:.2f}s")
        print(f"   ‚Ä¢ Temps de r√©ponse max : {max_response_time:.2f}s")
        
        # R√©sultats par test
        print(f"\nüîç R√©sultats d√©taill√©s par test :")
        for i, result in enumerate(self.test_results, 1):
            status = "‚úÖ" if result.get('overall_score', 0) >= 0.7 else "‚ùå"
            print(f"   {i}. {result['test_name']} : {status} (Score: {result.get('overall_score', 0):.2f})")
        
        # Recommandations
        print(f"\nüí° Recommandations :")
        if avg_precision < 0.8:
            print("   ‚Ä¢ Am√©liorer la pr√©cision : inclure tous les d√©tails attendus")
        if avg_tone < 3:
            print("   ‚Ä¢ Am√©liorer le ton humain : ajouter introductions et engagements")
        if avg_response_time > 2:
            print("   ‚Ä¢ Optimiser les performances : r√©duire le temps de r√©ponse")
        
        # Sauvegarde du rapport
        self.save_report()
    
    def save_report(self):
        """Sauvegarde le rapport au format JSON"""
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'test_results': self.test_results,
            'summary': {
                'total_tests': len(self.test_results),
                'successful_tests': len([r for r in self.test_results if 'error' not in r]),
                'failed_tests': len([r for r in self.test_results if 'error' in r])
            }
        }
        
        filename = f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            print(f"\nüíæ Rapport sauvegard√© : {filename}")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Erreur lors de la sauvegarde : {e}")

def main():
    """Fonction principale"""
    print("ü§ñ Suite de tests Botium pour le chatbot ParcInfo")
    print("Focus : Pr√©cision ET ton humain selon le nouveau prompt")
    print("=" * 60)
    
    # Cr√©er et ex√©cuter la suite de tests
    test_suite = BotiumTestSuite()
    success = test_suite.run_test_suite()
    
    if success:
        print("\nüéâ Suite de tests termin√©e avec succ√®s !")
    else:
        print("\n‚ùå Suite de tests √©chou√©e !")
    
    return success

if __name__ == "__main__":
    main()
