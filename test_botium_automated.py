#!/usr/bin/env python3
"""
Script de test automatisÃ© Botium-like pour le chatbot ParcInfo
Test des 9 questions identifiÃ©es comme nÃ©cessitant des amÃ©liorations
Focus : PrÃ©cision ET ton humain selon le nouveau prompt
"""

import os
import sys
import django
from datetime import datetime
import time
import json

# Configuration Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ParcInfo.settings')
django.setup()

from apps.chatbot.core_chatbot import ParcInfoChatbot

class BotiumTestSuite:
    """Suite de tests automatisÃ©s pour Ã©valuer la prÃ©cision et le ton humain"""
    
    def __init__(self):
        self.chatbot = None
        self.test_results = []
        self.start_time = None
        
    def setup(self):
        """Initialise le chatbot et la suite de tests"""
        try:
            print("ğŸš€ Initialisation de la suite de tests Botium...")
            self.chatbot = ParcInfoChatbot()
            print("âœ… Chatbot initialisÃ© avec succÃ¨s")
            return True
        except Exception as e:
            print(f"âŒ Erreur d'initialisation: {e}")
            return False
    
    def evaluate_precision(self, response: str, expected_elements: list) -> dict:
        """Ã‰value la prÃ©cision de la rÃ©ponse selon les Ã©lÃ©ments attendus"""
        score = 0
        found_elements = []
        missing_elements = []
        
        response_lower = response.lower()
        
        for element in expected_elements:
            if element.lower() in response_lower:
                score += 1
                found_elements.append(element)
            else:
                missing_elements.append(element)
        
        precision_score = score / len(expected_elements) if expected_elements else 0
        
        return {
            'score': precision_score,
            'found': found_elements,
            'missing': missing_elements,
            'is_accurate': precision_score >= 0.8
        }
    
    def evaluate_human_tone(self, response: str) -> dict:
        """Ã‰value le ton humain selon les directives du prompt"""
        score = 0
        issues = []
        improvements = []
        
        response_lower = response.lower()
        
        # VÃ©rifier l'introduction engageante (directive 2 du prompt)
        if any(greeting in response_lower for greeting in ['bonjour', 'salut', 'j\'ai vÃ©rifiÃ©', 'j\'ai trouvÃ©']):
            score += 2
        else:
            issues.append("Manque d'introduction engageante")
            improvements.append("Ajouter 'Bonjour ! J'ai vÃ©rifiÃ© pour vous'")
        
        # VÃ©rifier l'absence de termes techniques (directive 2 du prompt)
        technical_terms = ['it', 'cmd', 'fin', 'sÃ©rie', 'n/a']
        for term in technical_terms:
            if term in response_lower:
                score -= 1
                issues.append(f"Terme technique '{term}' prÃ©sent")
                improvements.append(f"Remplacer '{term}' par un terme clair")
        
        # VÃ©rifier l'invitation Ã  poursuivre (directive 2 du prompt)
        if any(engagement in response_lower for engagement in ['voulez-vous', 'avez-vous', 'besoin', 'souhaitez-vous']):
            score += 2
        else:
            issues.append("Manque d'invitation Ã  poursuivre")
            improvements.append("Ajouter une question d'engagement")
        
        # VÃ©rifier la structure claire (directive 4 du prompt)
        if len(response.split('.')) > 2:
            score += 1
        else:
            issues.append("Structure de rÃ©ponse trop simple")
            improvements.append("Structurer avec introduction, dÃ©tails, conclusion")
        
        # Normaliser le score sur 5
        score = max(0, min(5, score))
        
        return {
            'score': score,
            'issues': issues,
            'improvements': improvements,
            'is_human': score >= 3
        }
    
    def run_single_test(self, test_name: str, query: str, expected_elements: list, timeout: int = 5) -> dict:
        """ExÃ©cute un test individuel"""
        print(f"\nğŸ” Test : {test_name}")
        print(f"ğŸ“ Question : {query}")
        print("-" * 50)
        
        start_time = time.time()
        
        try:
            # ExÃ©cuter la requÃªte avec timeout
            response = self.chatbot.process_query(query)
            response_time = time.time() - start_time
            
            # Extraire le texte de la rÃ©ponse
            if isinstance(response, dict):
                response_text = response.get('response', 'N/A')
                intent = response.get('intent', 'N/A')
                confidence = response.get('confidence', 'N/A')
            else:
                response_text = response
                intent = 'N/A'
                confidence = 'N/A'
            
            # Ã‰valuer la prÃ©cision
            precision_eval = self.evaluate_precision(response_text, expected_elements)
            
            # Ã‰valuer le ton humain
            tone_eval = self.evaluate_human_tone(response_text)
            
            # VÃ©rifier le timeout
            timeout_ok = response_time < timeout
            
            # RÃ©sultats du test
            test_result = {
                'test_name': test_name,
                'query': query,
                'response': response_text,
                'intent': intent,
                'confidence': confidence,
                'response_time': response_time,
                'timeout_ok': timeout_ok,
                'precision': precision_eval,
                'tone': tone_eval,
                'overall_score': (precision_eval['score'] + tone_eval['score'] / 5) / 2
            }
            
            # Affichage des rÃ©sultats
            print(f"ğŸ“ RÃ©ponse : {response_text[:100]}{'...' if len(response_text) > 100 else ''}")
            print(f"ğŸ¯ Intent : {intent}")
            print(f"ğŸ“Š Confiance : {confidence}%")
            print(f"â±ï¸  Temps : {response_time:.2f}s")
            print(f"ğŸ“Š PrÃ©cision : {precision_eval['score']:.2f} ({'âœ…' if precision_eval['is_accurate'] else 'âŒ'})")
            print(f"ğŸ­ Ton humain : {tone_eval['score']}/5 ({'âœ…' if tone_eval['is_human'] else 'âŒ'})")
            
            if precision_eval['missing']:
                print(f"âš ï¸  Ã‰lÃ©ments manquants : {', '.join(precision_eval['missing'])}")
            
            if tone_eval['issues']:
                print(f"âš ï¸  ProblÃ¨mes de ton : {', '.join(tone_eval['issues'])}")
            
            if not timeout_ok:
                print(f"âš ï¸  Timeout dÃ©passÃ© ({response_time:.2f}s > {timeout}s)")
            
            return test_result
            
        except Exception as e:
            print(f"âŒ Erreur lors du test : {e}")
            return {
                'test_name': test_name,
                'query': query,
                'error': str(e),
                'overall_score': 0
            }
    
    def run_test_suite(self):
        """ExÃ©cute la suite complÃ¨te de tests"""
        if not self.setup():
            return False
        
        print("\nğŸ§ª DÃ©marrage de la suite de tests automatisÃ©s")
        print("=" * 60)
        
        self.start_time = datetime.now()
        
        # DÃ©finition des tests avec Ã©lÃ©ments attendus
        test_cases = [
            {
                'name': 'MatÃ©riels pour superadmin avec demandes',
                'query': "Quel matÃ©riel a Ã©tÃ© affectÃ© Ã  la demande de 'superadmin' ?",
                'expected': ['Demande nÂ°', 'cd14', 'sn14', 'BC23', 'CamÃ©ra', 'Casque', 'Baie']
            },
            {
                'name': 'MatÃ©riels informatiques sous garantie (dÃ©tails)',
                'query': "Quels matÃ©riels informatiques sont encore sous garantie ?",
                'expected': ['cd12', 'cd13', 'cd14', 'ADD/INFO/010', 'designation', 'utilisateur', 'numÃ©ro de sÃ©rie']
            },
            {
                'name': 'Garantie ADD/INFO/01094 (dÃ©tails)',
                'query': "La garantie du matÃ©riel avec le code d'inventaire ADD/INFO/01094 est-elle toujours active ?",
                'expected': ['ADD/INFO/01094', 'AOO2025', 'Armoire', 'jours restants', '28/07/2026']
            },
            {
                'name': 'MatÃ©riels bureautiques expirant bientÃ´t (contexte)',
                'query': "Liste les matÃ©riels bureautiques dont la garantie expire bientÃ´t.",
                'expected': ['30 jours', 'matÃ©riels actifs', 'contexte', 'ADD/INFO/01094']
            },
            {
                'name': 'Codes d\'inventaire Baie (utilisateurs)',
                'query': "Quel est le code d'inventaire de la Baie ?",
                'expected': ['cd12', 'cd13', 'cd14', 'utilisateur', 'employe anonyme', 'gestionnaire bureau', 'superadmin']
            },
            {
                'name': 'Commandes sans garantie (dates)',
                'query': "Y a-t-il des commandes sans garantie spÃ©cifiÃ©e ?",
                'expected': ['AOO2025', '123', 'BC23', 'expire', '28/07/2026', '15/09/2025', '23/08/2025']
            },
            {
                'name': 'Commandes bureautiques garantie annÃ©es (conversationnel)',
                'query': "Liste des commandes bureautiques avec garantie en annÃ©es.",
                'expected': ['AOO2025', '1 an', '28/07/2026', 'active', 'conversationnel']
            },
            {
                'name': 'Types de matÃ©riels (designations spÃ©cifiques)',
                'query': "Quels types de matÃ©riels sont disponibles ?",
                'expected': ['Baie', 'Call Server', 'Armoire', 'CÃ¢ble', 'cd12', 'cd13', 'cd14', 'ADD/INFO/010']
            },
            {
                'name': 'MatÃ©riel sn12 avec garantie (dÃ©tails)',
                'query': "Quel matÃ©riel informatique avec le numÃ©ro de sÃ©rie sn12 a une garantie associÃ©e ?",
                'expected': ['cd12', 'BC23', '23/08/2025', 'jours restants', '5 jours']
            },
            {
                'name': 'MatÃ©riels superadmin sous garantie (dÃ©tails)',
                'query': "Y a-t-il des matÃ©riels affectÃ©s Ã  'superadmin' encore sous garantie ?",
                'expected': ['cd14', 'sn14', 'Baie', '23/08/2025', '5 jours restants', 'BC23']
            }
        ]
        
        # ExÃ©cution des tests
        for test_case in test_cases:
            result = self.run_single_test(
                test_case['name'],
                test_case['query'],
                test_case['expected']
            )
            self.test_results.append(result)
        
        # GÃ©nÃ©ration du rapport
        self.generate_report()
        
        return True
    
    def generate_report(self):
        """GÃ©nÃ¨re un rapport dÃ©taillÃ© des tests"""
        print("\nğŸ“Š RAPPORT FINAL DES TESTS")
        print("=" * 60)
        
        if not self.test_results:
            print("âŒ Aucun rÃ©sultat de test disponible")
            return
        
        # Calcul des statistiques
        total_tests = len(self.test_results)
        successful_tests = len([r for r in self.test_results if 'error' not in r])
        failed_tests = total_tests - successful_tests
        
        # Scores moyens
        precision_scores = [r['precision']['score'] for r in self.test_results if 'precision' in r]
        tone_scores = [r['tone']['score'] for r in self.test_results if 'tone' in r]
        overall_scores = [r['overall_score'] for r in self.test_results if 'overall_score' in r]
        
        avg_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0
        avg_tone = sum(tone_scores) / len(tone_scores) if tone_scores else 0
        avg_overall = sum(overall_scores) / len(overall_scores) if overall_scores else 0
        
        # Temps de rÃ©ponse
        response_times = [r['response_time'] for r in self.test_results if 'response_time' in r]
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        
        # Affichage des statistiques
        print(f"ğŸ“ˆ Statistiques gÃ©nÃ©rales :")
        print(f"   â€¢ Tests exÃ©cutÃ©s : {total_tests}")
        print(f"   â€¢ Tests rÃ©ussis : {successful_tests}")
        print(f"   â€¢ Tests Ã©chouÃ©s : {failed_tests}")
        print(f"   â€¢ Taux de succÃ¨s : {(successful_tests/total_tests)*100:.1f}%")
        
        print(f"\nğŸ“Š Scores moyens :")
        print(f"   â€¢ PrÃ©cision : {avg_precision:.2f}/1.0")
        print(f"   â€¢ Ton humain : {avg_tone:.1f}/5.0")
        print(f"   â€¢ Score global : {avg_overall:.2f}/1.0")
        
        print(f"\nâ±ï¸  Performance :")
        print(f"   â€¢ Temps de rÃ©ponse moyen : {avg_response_time:.2f}s")
        print(f"   â€¢ Temps de rÃ©ponse max : {max_response_time:.2f}s")
        
        # RÃ©sultats par test
        print(f"\nğŸ” RÃ©sultats dÃ©taillÃ©s par test :")
        for i, result in enumerate(self.test_results, 1):
            status = "âœ…" if result.get('overall_score', 0) >= 0.7 else "âŒ"
            print(f"   {i}. {result['test_name']} : {status} (Score: {result.get('overall_score', 0):.2f})")
        
        # Recommandations
        print(f"\nğŸ’¡ Recommandations :")
        if avg_precision < 0.8:
            print("   â€¢ AmÃ©liorer la prÃ©cision : inclure tous les dÃ©tails attendus")
        if avg_tone < 3:
            print("   â€¢ AmÃ©liorer le ton humain : ajouter introductions et engagements")
        if avg_response_time > 2:
            print("   â€¢ Optimiser les performances : rÃ©duire le temps de rÃ©ponse")
        
        # Sauvegarde du rapport
        self.save_report()
    
    def save_report(self):
        """Sauvegarde le rapport au format JSON"""
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'test_results': self.test_results,
            'summary': {
                'total_tests': len(self.test_results),
                'successful_tests': len([r for r in self.test_results if 'error' not in r]),
                'failed_tests': len([r for r in self.test_results if 'error' in r])
            }
        }
        
        filename = f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ Rapport sauvegardÃ© : {filename}")
        except Exception as e:
            print(f"\nâš ï¸  Erreur lors de la sauvegarde : {e}")

def main():
    """Fonction principale"""
    print("ğŸ¤– Suite de tests Botium pour le chatbot ParcInfo")
    print("Focus : PrÃ©cision ET ton humain selon le nouveau prompt")
    print("=" * 60)
    
    # CrÃ©er et exÃ©cuter la suite de tests
    test_suite = BotiumTestSuite()
    success = test_suite.run_test_suite()
    
    if success:
        print("\nğŸ‰ Suite de tests terminÃ©e avec succÃ¨s !")
    else:
        print("\nâŒ Suite de tests Ã©chouÃ©e !")
    
    return success

if __name__ == "__main__":
    main()
