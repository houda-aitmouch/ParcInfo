#!/usr/bin/env python3
"""
Script de test automatis√© pour les 10 questions compl√©mentaires du chatbot ParcInfo
Cible : Pr√©cision, compl√©tude, ton humain, robustesse face aux erreurs
Date : 18/08/2025, 21:30 +01
"""

import os
import sys
import django
from datetime import datetime
import time

# Configuration Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ParcInfo.settings')
django.setup()

from apps.chatbot.core_chatbot import ParcInfoChatbot

class TestQuestionsComplementaires:
    """Test des 10 questions compl√©mentaires pour valider les am√©liorations"""
    
    def __init__(self):
        print("üß™ Test des Questions Compl√©mentaires du Chatbot ParcInfo")
        print("=" * 60)
        print("Focus : Pr√©cision, Compl√©tude, Ton Humain, Robustesse")
        print("Date : 18/08/2025, 21:30 +01")
        print()
        
        try:
            self.chatbot = ParcInfoChatbot()
            print("‚úÖ Chatbot initialis√© avec succ√®s")
        except Exception as e:
            print(f"‚ùå Erreur d'initialisation : {e}")
            sys.exit(1)
    
    def evaluate_response_quality(self, response: str, expected_keywords: list, 
                                expected_absence: list = None) -> dict:
        """√âvalue la qualit√© de la r√©ponse"""
        if expected_absence is None:
            expected_absence = []
        
        # V√©rification des mots-cl√©s attendus
        keywords_found = []
        for keyword in expected_keywords:
            if keyword.lower() in response.lower():
                keywords_found.append(keyword)
        
        # V√©rification de l'absence de mots non d√©sir√©s
        unwanted_found = []
        for word in expected_absence:
            if word.lower() in response.lower():
                unwanted_found.append(word)
        
        # Score de pr√©cision
        precision_score = len(keywords_found) / len(expected_keywords) if expected_keywords else 0
        
        # Score de ton humain
        human_indicators = [
            'bonjour', 'salut', 'excellent', 'parfait', 'super',
            'veux-tu', 'besoin', 'd√©sol√©', 'd√©sol√©e', 'merci'
        ]
        human_score = sum(1 for indicator in human_indicators if indicator in response.lower()) / len(human_indicators)
        
        return {
            'precision_score': precision_score,
            'keywords_found': keywords_found,
            'missing_keywords': [k for k in expected_keywords if k not in keywords_found],
            'unwanted_found': unwanted_found,
            'human_score': human_score,
            'response_length': len(response)
        }
    
    def test_question(self, question: str, expected_keywords: list, 
                      expected_absence: list = None, description: str = "") -> dict:
        """Teste une question sp√©cifique"""
        print(f"üîç Question : {question}")
        if description:
            print(f"üìã Description : {description}")
        print("-" * 50)
        
        start_time = time.time()
        try:
            response = self.chatbot.process_query(question)
            response_time = time.time() - start_time
        except Exception as e:
            response = f"Erreur : {e}"
            response_time = 0
        
        # G√©rer les r√©ponses dict vs string
        response_text = response.get('response', str(response)) if isinstance(response, dict) else str(response)
        
        print(f"üìù R√©ponse : {response_text}")
        print(f"‚è±Ô∏è  Temps : {response_time:.2f}s")
        
        # √âvaluation de la qualit√©
        quality = self.evaluate_response_quality(response_text, expected_keywords, expected_absence)
        
        print(f"üéØ Score pr√©cision : {quality['precision_score']:.1%}")
        print(f"‚úÖ Mots-cl√©s trouv√©s : {', '.join(quality['keywords_found'])}")
        if quality['missing_keywords']:
            print(f"‚ùå Mots-cl√©s manquants : {', '.join(quality['missing_keywords'])}")
        if quality['unwanted_found']:
            print(f"‚ö†Ô∏è  Mots non d√©sir√©s : {', '.join(quality['unwanted_found'])}")
        print(f"üé≠ Score ton humain : {quality['human_score']:.1%}")
        print(f"üìè Longueur r√©ponse : {quality['response_length']} caract√®res")
        
        # V√©rification performance
        if response_time > 2.0:
            print("‚ö†Ô∏è  Performance : R√©ponse lente (>2s)")
        else:
            print("‚úÖ Performance : R√©ponse rapide (<2s)")
        
        print()
        return {
            'question': question,
            'response': response,
            'response_time': response_time,
            'quality': quality
        }
    
    def run_all_tests(self):
        """Ex√©cute tous les tests des questions compl√©mentaires"""
        print("üìã Test de 10 questions compl√©mentaires")
        print("Cible : Lacunes identifi√©es (pr√©cision, compl√©tude, ton, robustesse)")
        print("-" * 50)
        print()
        
        test_cases = [
            {
                'question': "Quelles sont les demandes en attente pour superadmin ?",
                'expected_keywords': ['aucune demande', 'n¬∞11', 'n¬∞21', 'n¬∞23', 'approuv√©es'],
                'expected_absence': ['en attente'],
                'description': "V√©rifier absence de demandes en attente et liste des demandes approuv√©es"
            },
            {
                'question': "Quel est le fourniseur de la comande BC23 ?",
                'expected_keywords': ['TECHNICOVIGILE', 'BC23', 'cd12', 'cd13', 'cd14'],
                'expected_absence': ['fourniseur', 'comande'],
                'description': "Tester robustesse face aux fautes d'orthographe et pr√©cision des donn√©es"
            },
            {
                'question': "Quels mat√©riels ont √©t√© livr√©s apr√®s la date pr√©vue ?",
                'expected_keywords': ['retard', 'BC23', '123', 'AOO2025', 'cd12', 'cd13', 'cd14'],
                'description': "V√©rifier la gestion des retards de livraison et la pr√©cision des dates"
            },
            {
                'question': "Combien de mat√©riels sont en maintenance actuellement ?",
                'expected_keywords': ['aucun mat√©riel', 'maintenance', 'nouveaux', 'affect√©s'],
                'description': "Tester la gestion des cas vides et la fourniture de contexte"
            },
            {
                'question': "Quels mat√©riels sont associ√©s √† la comande 123 ?",
                'expected_keywords': ['commande 123', 'ADD/INFO/010', 'ADD/INFO/01000', '15/09/2025'],
                'expected_absence': ['comande'],
                'description': "V√©rifier la correction des fautes et la compl√©tude des informations"
            },
            {
                'question': "Quel est le mat√©riel avec le num√©ro de s√©rie 123456 ?",
                'expected_keywords': ['num√©ro de s√©rie 123456', 'incorrect', 'ADD/INFO/010'],
                'description': "Tester la gestion des donn√©es inexistantes et la fourniture d'alternatives"
            },
            {
                'question': "Quelles sont les commandes pass√©es par le fourniseur AEBDM ?",
                'expected_keywords': ['AEBDM', 'AOO2025', 'ADD/INFO/01094', '28/07/2025'],
                'expected_absence': ['fourniseur'],
                'description': "V√©rifier la gestion des fautes et la pr√©cision des relations fournisseur-commande"
            },
            {
                'question': "Combien de mat√©riels informatiques sont affect√©s √† un utilisateur ?",
                'expected_keywords': ['3 mat√©riels', 'cd12', 'cd13', 'cd14', 'employe anonyme', 'gestionnaire bureau', 'superadmin'],
                'description': "Tester les statistiques et la compl√©tude des informations utilisateur"
            },
            {
                'question': "Quels mat√©riels ont une garantie de moins de 10 jours restants ?",
                'expected_keywords': ['moins de 10 jours', 'cd12', 'cd13', 'cd14', '5 jours restants', '23/08/2025'],
                'description': "V√©rifier le calcul des jours restants et la pr√©cision des dates"
            },
            {
                'question': "Quel mat√©riel est li√© √† la demande n¬∞999 ?",
                'expected_keywords': ['demande n¬∞999', 'n\'existe pas', 'n¬∞11', 'n¬∞21', 'n¬∞23'],
                'description': "Tester la gestion des cas limites et la fourniture d'alternatives"
            }
        ]
        
        results = []
        for i, test_case in enumerate(test_cases, 1):
            print(f"üìù Test {i}/10")
            result = self.test_question(
                test_case['question'],
                test_case['expected_keywords'],
                test_case.get('expected_absence', []),
                test_case['description']
            )
            results.append(result)
        
        return results
    
    def generate_report(self, results: list):
        """G√©n√®re un rapport d√©taill√© des tests"""
        print("üèÅ Rapport des Tests Compl√©mentaires")
        print("=" * 60)
        
        # Statistiques globales
        total_questions = len(results)
        avg_precision = sum(r['quality']['precision_score'] for r in results) / total_questions
        avg_human_score = sum(r['quality']['human_score'] for r in results) / total_questions
        avg_response_time = sum(r['response_time'] for r in results) / total_questions
        
        print(f"üìä Statistiques Globales :")
        print(f"   ‚Ä¢ Questions test√©es : {total_questions}")
        print(f"   ‚Ä¢ Score pr√©cision moyen : {avg_precision:.1%}")
        print(f"   ‚Ä¢ Score ton humain moyen : {avg_human_score:.1%}")
        print(f"   ‚Ä¢ Temps de r√©ponse moyen : {avg_response_time:.2f}s")
        print()
        
        # Questions avec probl√®mes
        problematic_questions = [r for r in results if r['quality']['precision_score'] < 0.8 or r['quality']['human_score'] < 0.6]
        
        if problematic_questions:
            print("‚ö†Ô∏è  Questions avec Probl√®mes :")
            print("-" * 40)
            for result in problematic_questions:
                print(f"   ‚Ä¢ {result['question'][:50]}...")
                if result['quality']['precision_score'] < 0.8:
                    print(f"     - Pr√©cision faible : {result['quality']['precision_score']:.1%}")
                if result['quality']['human_score'] < 0.6:
                    print(f"     - Ton non-humain : {result['quality']['human_score']:.1%}")
                if result['response_time'] > 2.0:
                    print(f"     - Performance lente : {result['response_time']:.2f}s")
            print()
        
        # Questions r√©ussies
        successful_questions = [r for r in results if r['quality']['precision_score'] >= 0.8 and r['quality']['human_score'] >= 0.6 and r['response_time'] <= 2.0]
        
        if successful_questions:
            print("‚úÖ Questions R√©ussies :")
            print("-" * 30)
            for result in successful_questions:
                print(f"   ‚Ä¢ {result['question'][:50]}...")
            print()
        
        # Recommandations
        print("üí° Recommandations :")
        print("-" * 20)
        
        if avg_precision < 0.8:
            print("   ‚Ä¢ Am√©liorer la pr√©cision des r√©ponses (cible : >80%)")
        if avg_human_score < 0.6:
            print("   ‚Ä¢ Am√©liorer le ton humain (cible : >60%)")
        if avg_response_time > 2.0:
            print("   ‚Ä¢ Optimiser les performances (cible : <2s)")
        
        if problematic_questions:
            print("   ‚Ä¢ Revoir les questions probl√©matiques identifi√©es")
        
        print("   ‚Ä¢ Continuer les tests avec variations et cas limites")
        print("   ‚Ä¢ Impl√©menter le feedback utilisateur (thumbs up/down)")
        
        return {
            'total_questions': total_questions,
            'avg_precision': avg_precision,
            'avg_human_score': avg_human_score,
            'avg_response_time': avg_response_time,
            'problematic_count': len(problematic_questions),
            'successful_count': len(successful_questions)
        }

def main():
    """Fonction principale"""
    try:
        tester = TestQuestionsComplementaires()
        results = tester.run_all_tests()
        report = tester.generate_report(results)
        
        print("üéØ Objectifs de Performance :")
        print(f"   ‚Ä¢ Pr√©cision >95% : {'‚úÖ Atteint' if report['avg_precision'] >= 0.95 else '‚ùå Non atteint'} ({report['avg_precision']:.1%})")
        print(f"   ‚Ä¢ Ton humain CSAT >4/5 : {'‚úÖ Atteint' if report['avg_human_score'] >= 0.8 else '‚ùå Non atteint'} ({report['avg_human_score']:.1%})")
        print(f"   ‚Ä¢ Performance <2s : {'‚úÖ Atteint' if report['avg_response_time'] <= 2.0 else '‚ùå Non atteint'} ({report['avg_response_time']:.2f}s)")
        
        print("\nüöÄ Test des questions compl√©mentaires termin√© !")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Test interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur lors du test : {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
