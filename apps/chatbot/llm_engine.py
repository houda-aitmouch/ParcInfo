import os
import json
import warnings
warnings.filterwarnings("ignore", message="Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/")
from typing import List, Dict, Any

# Core LangChain
import langchain
import warnings
from langchain.memory import ConversationBufferMemory
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# Modules communautaires
from langchain_community.llms import LlamaCpp
from langchain_community.vectorstores import Chroma

# Import recommand√© pour HuggingFacePipeline et HuggingFaceEmbeddings directement depuis langchain_huggingface
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings

# Transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from django.db import connection
from django.contrib.auth import get_user_model

import pandas as pd
User = get_user_model()

class ParcInfoChatbot:
    """Chatbot IA avanc√© pour le syst√®me ParcInfo avec apprentissage continu"""

    def __init__(self):
        self.llm = None
        self.vectorstore = None
        self.chain = None
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            input_key="question",
            output_key="answer"
        )
        self.conversation_history = []
        self.initialize_llm()
        self.initialize_vectorstore()
        self.initialize_chain()

    def initialize_llm(self):
        """Initialiser le mod√®le LLaMA 3"""
        try:
            model_path = os.getenv('LLAMA_MODEL_PATH', 'models/llama-3-8b-instruct.gguf')

            if os.path.exists(model_path):
                self.llm = LlamaCpp(
                    model_path=model_path,
                    temperature=0.7,
                    max_tokens=2048,
                    top_p=1,
                    verbose=True,
                    n_ctx=4096,
                    repeat_penalty=1.1
                )
                print(f"‚úÖ LLaMA 3 initialis√©: {model_path}")
            else:
                model_name = "microsoft/DialoGPT-medium"
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForCausalLM.from_pretrained(model_name)

                pipe = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    max_length=2048,
                    temperature=0.7,
                    do_sample=True
                )

                self.llm = HuggingFacePipeline(pipeline=pipe)
                print(f"‚úÖ HuggingFace Pipeline: {model_name}")

        except Exception as e:
            print(f"‚ùå Erreur LLM: {e}")
            self.llm = None

    def get_comprehensive_database_context(self) -> str:
        """R√©cup√©rer le contexte complet et d√©taill√© de la base de donn√©es"""
        context_parts = []

        try:
            with connection.cursor() as cursor:
                # Statistiques globales
                cursor.execute("""
                    SELECT 
                        COUNT(*) as total_users,
                        COUNT(CASE WHEN is_superuser THEN 1 END) as superusers,
                        COUNT(CASE WHEN is_staff THEN 1 END) as staff,
                        COUNT(CASE WHEN NOT is_superuser AND NOT is_staff THEN 1 END) as employees
                    FROM users_customuser
                """)
                user_stats = cursor.fetchone()
                context_parts.append(f"Utilisateurs: {user_stats[0]} total ({user_stats[1]} superadmin, {user_stats[2]} staff, {user_stats[3]} employ√©s)")

                # Demandes d'√©quipement d√©taill√©es
                cursor.execute("""
                    SELECT statut, categorie, type_demande, COUNT(*) as count
                    FROM demande_equipement_demandeequipement
                    GROUP BY statut, categorie, type_demande
                    ORDER BY count DESC
                """)
                demandes = cursor.fetchall()
                if demandes:
                    context_parts.append("Demandes d'√©quipement:")
                    for statut, categorie, type_demande, count in demandes:
                        context_parts.append(f"- {categorie} {type_demande}: {count} ({statut})")

                # Mat√©riel informatique
                cursor.execute("""
                    SELECT statut, lieu_stockage, COUNT(*) as count
                    FROM materiel_informatique_materielinformatique
                    GROUP BY statut, lieu_stockage
                    ORDER BY count DESC
                """)
                materiel_info = cursor.fetchall()
                if materiel_info:
                    context_parts.append("Mat√©riel informatique:")
                    for statut, lieu, count in materiel_info:
                        context_parts.append(f"- {statut}: {count} ({lieu})")

                # Mat√©riel bureautique
                cursor.execute("""
                    SELECT statut, lieu_stockage, COUNT(*) as count
                    FROM materiel_bureautique_materielbureau
                    GROUP BY statut, lieu_stockage
                    ORDER BY count DESC
                """)
                materiel_bureau = cursor.fetchall()
                if materiel_bureau:
                    context_parts.append("Mat√©riel bureautique:")
                    for statut, lieu, count in materiel_bureau:
                        context_parts.append(f"- {statut}: {count} ({lieu})")

                # Livraisons
                cursor.execute("""
                    SELECT statut_livraison, type_commande, COUNT(*) as count
                    FROM livraison_livraison
                    GROUP BY statut_livraison, type_commande
                    ORDER BY count DESC
                """)
                livraisons = cursor.fetchall()
                if livraisons:
                    context_parts.append("Livraisons:")
                    for statut, type_commande, count in livraisons:
                        context_parts.append(f"- {type_commande}: {count} ({statut})")

                # Fournisseurs
                cursor.execute("""
                    SELECT COUNT(*) as count
                    FROM fournisseurs_fournisseur
                """)
                fournisseurs_count = cursor.fetchone()[0]
                context_parts.append(f"Fournisseurs: {fournisseurs_count}")

        except Exception as e:
            context_parts.append(f"Erreur base de donn√©es: {e}")

        return "\n".join(context_parts)

    def get_user_context(self, user: User) -> str:
        """R√©cup√©rer le contexte sp√©cifique √† l'utilisateur"""
        user_context = []

        try:
            with connection.cursor() as cursor:
                # R√¥le de l'utilisateur
                role = "Super Admin" if user.is_superuser else "Gestionnaire" if user.is_staff else "Employ√©"
                user_context.append(f"R√¥le: {role}")

                # Demandes g√©r√©es par l'utilisateur
                cursor.execute("""
                    SELECT statut, categorie, type_demande, COUNT(*) as count
                    FROM demande_equipement_demandeequipement
                    WHERE demandeur_id = %s
                    GROUP BY statut, categorie, type_demande
                    ORDER BY count DESC
                """, [user.id])
                user_demandes = cursor.fetchall()
                if user_demandes:
                    user_context.append("Demandes personnelles:")
                    for statut, categorie, type_demande, count in user_demandes:
                        user_context.append(f"- {categorie} {type_demande}: {count} ({statut})")

                # Mat√©riel g√©r√© par l'utilisateur
                cursor.execute("""
                    SELECT statut, COUNT(*) as count
                    FROM materiel_informatique_materielinformatique
                    WHERE utilisateur_id = %s
                    GROUP BY statut
                """, [user.id])
                user_materiel_info = cursor.fetchall()
                if user_materiel_info:
                    user_context.append("Mat√©riel informatique g√©r√©:")
                    for statut, count in user_materiel_info:
                        user_context.append(f"- {statut}: {count}")

                cursor.execute("""
                    SELECT statut, COUNT(*) as count
                    FROM materiel_bureautique_materielbureau
                    WHERE utilisateur_id = %s
                    GROUP BY statut
                """, [user.id])
                user_materiel_bureau = cursor.fetchall()
                if user_materiel_bureau:
                    user_context.append("Mat√©riel bureautique g√©r√©:")
                    for statut, count in user_materiel_bureau:
                        user_context.append(f"- {statut}: {count}")

        except Exception as e:
            user_context.append(f"Erreur contexte utilisateur: {e}")

        return "\n".join(user_context) if user_context else "Aucune donn√©e personnelle"

    def get_system_analytics(self) -> str:
        """R√©cup√©rer des analyses syst√®me avanc√©es"""
        analytics = []

        try:
            with connection.cursor() as cursor:
                # Tendances des demandes
                cursor.execute("""
                    SELECT 
                        DATE(date_demande) as date,
                        COUNT(*) as count,
                        AVG(CASE WHEN statut = 'approuvee' THEN 1 ELSE 0 END) as approval_rate
                    FROM demande_equipement_demandeequipement
                    WHERE date_demande >= CURRENT_DATE - INTERVAL '30 days'
                    GROUP BY DATE(date_demande)
                    ORDER BY date DESC
                    LIMIT 10
                """)
                trends = cursor.fetchall()
                if trends:
                    analytics.append("Tendances r√©centes (30 jours):")
                    for date, count, approval_rate in trends:
                        analytics.append(f"- {date}: {count} demandes ({approval_rate*100:.1f}% approuv√©es)")

                # Performance du syst√®me
                cursor.execute("""
                    SELECT 
                        COUNT(*) as total_demandes,
                        COUNT(CASE WHEN statut = 'approuvee' THEN 1 END) as approuvees,
                        COUNT(CASE WHEN statut = 'refusee' THEN 1 END) as refusees,
                        COUNT(CASE WHEN statut = 'en_attente' THEN 1 END) as en_attente
                    FROM demande_equipement_demandeequipement
                """)
                perf = cursor.fetchone()
                if perf:
                    total, approuvees, refusees, en_attente = perf
                    if total > 0:
                        analytics.append(f"Performance: {approuvees}/{total} approuv√©es ({approuvees/total*100:.1f}%)")
                        analytics.append(f"En attente: {en_attente}, Refus√©es: {refusees}")

        except Exception as e:
            analytics.append(f"Erreur analytics: {e}")

        return "\n".join(analytics)

    def initialize_vectorstore(self):
        """Initialiser le vectorstore avec contexte enrichi"""
        try:
            documents = []

            # Documentation syst√®me compl√®te
            system_doc = """
            Syst√®me ParcInfo - Gestion Avanc√©e de Parc Informatique et Bureautique
            
            ARCHITECTURE DU SYST√àME:
            - Gestion centralis√©e des √©quipements informatiques et bureautiques
            - Workflow de demandes automatis√© avec approbation hi√©rarchique
            - Suivi complet du cycle de vie des √©quipements
            - Int√©gration fournisseurs et livraisons
            - Reporting et analytics en temps r√©el
            
            R√îLES UTILISATEURS:
            - Super Admin: Acc√®s complet, gestion utilisateurs, configuration syst√®me
            - Gestionnaire Informatique: Gestion mat√©riel informatique, approbation demandes
            - Gestionnaire Bureau: Gestion mat√©riel bureautique, approbation demandes
            - Employ√©: Soumission demandes, consultation mat√©riel assign√©
            
            WORKFLOW DE DEMANDE:
            1. Soumission par l'employ√© (categorie, type_demande, designation)
            2. Validation automatique des r√®gles m√©tier
            3. Notification aux gestionnaires concern√©s
            4. Approbation/refus avec justifications
            5. Affectation mat√©riel si approuv√©e
            6. Signature d√©charge √©lectronique
            7. Livraison et installation
            8. Suivi maintenance et garantie
            
            STATUTS ET TRANSITIONS:
            Demandes: en_attente ‚Üí approuvee/refusee ‚Üí affectee ‚Üí livree
            Mat√©riel: nouveau ‚Üí affecte ‚Üí operationnel ‚Üí maintenance ‚Üí reforme
            Livraisons: en_attente ‚Üí en_cours ‚Üí livree ‚Üí validee
            
            M√âTRIQUES CL√âS:
            - Taux d'approbation des demandes
            - Temps de traitement moyen
            - Taux d'utilisation du mat√©riel
            - Co√ªts par cat√©gorie d'√©quipement
            - Satisfaction utilisateur
            """

            documents.append(Document(page_content=system_doc, metadata={"source": "system_architecture"}))

            # Contexte base de donn√©es
            db_context = self.get_comprehensive_database_context()
            if db_context:
                documents.append(Document(page_content=f"√âtat syst√®me actuel:\n{db_context}", metadata={"source": "database_state"}))

            # Analytics syst√®me
            analytics = self.get_system_analytics()
            if analytics:
                documents.append(Document(page_content=f"Analytics syst√®me:\n{analytics}", metadata={"source": "system_analytics"}))

            # Initialiser embeddings et vectorstore
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'}
            )

            if documents:
                self.vectorstore = Chroma.from_documents(
                    documents=documents,
                    embedding=embeddings,
                    persist_directory="./chroma_db"
                )
                print("‚úÖ Vectorstore enrichi initialis√©")

        except Exception as e:
            print(f"‚ùå Erreur vectorstore: {e}")
            self.vectorstore = None

    def initialize_chain(self):
        """Initialiser la cha√Æne de conversation avec apprentissage"""
        if not self.llm or not self.vectorstore:
            print("‚ùå Impossible d'initialiser la cha√Æne")
            return

        try:
            # Prompt optimis√© pour apprentissage continu
            template = """
            Tu es l'assistant IA ParcInfo, un expert en gestion de parc informatique et bureautique.
            
            CONTEXTE SYST√àME:
            {context}
            
            HISTORIQUE CONVERSATION:
            {chat_history}
            
            QUESTION UTILISATEUR: {question}
            
            DIRECTIVES:
            1. R√©ponds en fran√ßais professionnel et technique
            2. Utilise les donn√©es du contexte pour des r√©ponses pr√©cises
            3. Fournis des insights bas√©s sur les analytics
            4. Sugg√®re des am√©liorations et optimisations
            5. Explique les processus et workflows
            6. Donne des recommandations d'action
            7. Adapte tes r√©ponses au r√¥le de l'utilisateur
            8. Apprends des interactions pr√©c√©dentes pour am√©liorer tes r√©ponses
            
            R√âPONSE:
            """

            prompt = PromptTemplate(
                input_variables=["context", "chat_history", "question"],
                template=template
            )

            self.chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=self.vectorstore.as_retriever(search_kwargs={"k": 7}),
                memory=self.memory,
                combine_docs_chain_kwargs={"prompt": prompt},
                return_source_documents=True,
                verbose=True
            )

            print("‚úÖ Cha√Æne avec apprentissage initialis√©e")

        except Exception as e:
            print(f"‚ùå Erreur cha√Æne: {e}")

    def get_response(self, question: str, user: User = None) -> str:
        """G√©n√©rer une r√©ponse IA avec apprentissage continu"""
        try:
            if not self.chain:
                return "Syst√®me IA temporairement indisponible. Contactez l'administrateur."

            # Construire le contexte enrichi
            user_context = ""
            if user:
                user_data = self.get_user_context(user)
                user_context = f"\n\nCONTEXTE UTILISATEUR ({user.username}):\n{user_data}"

            # Question enrichie avec contexte
            enriched_question = f"{question}{user_context}"

            # G√©n√©rer r√©ponse via LangChain
            response = self.chain({"question": enriched_question})
            answer = response.get('answer', '')

            # Validation et am√©lioration de la r√©ponse
            if not answer or len(answer.strip()) < 20:
                # Tentative de reformulation
                reformulated_question = f"Question technique ParcInfo: {question}"
                response = self.chain({"question": reformulated_question})
                answer = response.get('answer', '')

            # R√©ponse de secours si n√©cessaire
            if not answer or len(answer.strip()) < 20:
                answer = f"Je traite votre question sur '{question}'. Pour une r√©ponse optimale, pourriez-vous pr√©ciser votre demande ? Je peux analyser les donn√©es syst√®me, fournir des insights, et vous guider dans la gestion du parc."

            # Apprentissage: sauvegarder l'interaction
            self.learn_from_interaction(question, answer, user)

            return answer

        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration r√©ponse: {e}")
            return "Erreur technique. Le syst√®me IA est en cours de r√©cup√©ration."

    def learn_from_interaction(self, question: str, answer: str, user: User = None):
        """Apprentissage continu bas√© sur les interactions"""
        try:
            # Sauvegarder l'interaction pour apprentissage
            interaction = {
                'question': question,
                'answer': answer,
                'user_role': 'Super Admin' if user and user.is_superuser else 'Gestionnaire' if user and user.is_staff else 'Employ√©',
                'timestamp': pd.Timestamp.now(),
                'context_length': len(answer)
            }

            self.conversation_history.append(interaction)

            # Limiter l'historique pour √©viter la surcharge m√©moire
            if len(self.conversation_history) > 1000:
                self.conversation_history = self.conversation_history[-500:]

            # Rafra√Æchir le contexte p√©riodiquement
            if len(self.conversation_history) % 50 == 0:
                self.refresh_context()

        except Exception as e:
            print(f"‚ùå Erreur apprentissage: {e}")

    def refresh_context(self):
        """Rafra√Æchir le contexte avec nouvelles donn√©es"""
        try:
            print("üîÑ Rafra√Æchissement contexte IA...")
            self.initialize_vectorstore()
            self.initialize_chain()
            print("‚úÖ Contexte rafra√Æchi")
        except Exception as e:
            print(f"‚ùå Erreur rafra√Æchissement: {e}")

    def get_learning_insights(self) -> Dict[str, Any]:
        """Obtenir des insights sur l'apprentissage du chatbot"""
        if not self.conversation_history:
            return {"message": "Aucune donn√©e d'apprentissage disponible"}

        try:
            df = pd.DataFrame(self.conversation_history)

            insights = {
                'total_interactions': len(df),
                'avg_response_length': df['context_length'].mean(),
                'user_role_distribution': df['user_role'].value_counts().to_dict(),
                'recent_activity': len(df[df['timestamp'] > pd.Timestamp.now() - pd.Timedelta(days=7)]),
                'learning_progress': "Actif" if len(df) > 10 else "En cours d'initialisation"
            }

            return insights

        except Exception as e:
            return {"error": f"Erreur insights: {e}"}